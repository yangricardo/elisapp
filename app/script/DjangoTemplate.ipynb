{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urllib3.connectionpool DEBUG    Starting new HTTP connection (1): 0.0.0.0:8000\n",
      "urllib3.connectionpool DEBUG    http://0.0.0.0:8000 \"POST /auth/login HTTP/1.1\" 200 151\n",
      "urllib3.connectionpool DEBUG    Starting new HTTP connection (1): 0.0.0.0:8000\n",
      "urllib3.connectionpool DEBUG    http://0.0.0.0:8000 \"GET /api HTTP/1.1\" 301 0\n",
      "urllib3.connectionpool DEBUG    Resetting dropped connection: 0.0.0.0\n",
      "urllib3.connectionpool DEBUG    http://0.0.0.0:8000 \"GET /api/ HTTP/1.1\" 200 721\n"
     ]
    }
   ],
   "source": [
    "import os, sys, django, pandas as pd , numpy as np, requests, json, re\n",
    "from utils import TJData, setup_env, APIHandler, nan_to_int\n",
    "# configura notebook para se utilizar recursos do django, se necessário\n",
    "setup_env() \n",
    "from api import models as tj\n",
    "# objeto para interagir com a minha API\n",
    "api = APIHandler('http://0.0.0.0:8000','elisdbadmin','elisdbpassword')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comarca\n",
    "O bloco de código abaixo foi utilizado para analisar e decidir visualmente qual planilha referente a comarcas será utilizada, a planilha mapeada por `comarca` possui a coluna `COD_SERV` possui codigos distintos dos referenciados por demais planilhas\n",
    "```\n",
    "com_1 = TJData.read_csv(TJData.csv['comarca'],usecols=['COD_COM','NOME','NOME_REDU','COD_TJ'])\n",
    "com_2 = TJData.read_csv(TJData.csv['comarca_2'], usecols=['COD_COMA','DESC_COMA','COD_TJ','DESC_REDU'])\n",
    "com = pd.merge(left=com_1,right=com_2,how='inner',left_on='NOME_REDU', right_on='DESC_REDU')\n",
    "```\n",
    "\n",
    "#### Tratamento de dados"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# comarcas = TJData.read_csv(TJData.csv['comarca_2'], usecols=['COD_COMA','DESC_COMA','DESC_REDU'])\n",
    "# filtra as linhas com cod_coma nulas\n",
    "comarcas = comarcas[~pd.isnull(comarcas.COD_COMA)]\n",
    "# converte cod_coma  para inteiro\n",
    "comarcas.COD_COMA = comarcas.COD_COMA.apply(lambda x: int(x))\n",
    "# converte o cabeçalho para caixa baixa\n",
    "comarcas = comarcas.rename(columns=lambda x: str(x).lower())\n",
    "# gera lista de objetos json a partir do dataframe\n",
    "comarca_json = json.loads(comarcas.to_json(orient='records'))\n",
    "# executa chamada a api\n",
    "r = api.post('comarcas',comarca_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serventia\n",
    "\n",
    "``` \n",
    "serv_1 = TJData.read_csv(TJData.csv['serventia_2'], usecols=['COD_SERV','DESC_SERV','COD_COMA','DESC_ABRE','DESC_REDU'] )\n",
    "serv_2 = TJData.read_csv(TJData.csv['serventias_full'], usecols=['COD_SERV','DESC_SERV','COD_COMA','DESC_ABRE','DESC_REDU'])\n",
    "serv = pd.merge(left=serv_1,right=serv_2,how='inner',left_on='COD_SERV', right_on='COD_SERV')\n",
    "serv_outer = pd.merge(left=serv_1,right=serv_2,how='outer',left_on='COD_SERV', right_on='COD_SERV')\n",
    "```\n",
    "#### Tratamento de dados"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "serventias = TJData.read_csv(TJData.csv['serventia_2'], usecols=['COD_SERV','DESC_SERV','COD_COMA','DESC_ABRE'] )\n",
    "serventias = serventias[~pd.isnull(serventias.COD_COMA)]\n",
    "serventias.COD_COMA = serventias.COD_COMA.apply(lambda x: int(x))\n",
    "# elimina codigos não referenciados\n",
    "serventias = serventias[serventias.COD_COMA != 888]\n",
    "serventias = serventias[serventias.COD_COMA != 999]\n",
    "serventias = serventias.rename(columns=lambda x: str(x).lower())\n",
    "# ajusta cod_coma para referenciar o atributo comarca(FK) de serventia\n",
    "serventias = serventias.rename(columns={'cod_coma':'comarca'})\n",
    "serventia_json = json.loads(serventias.to_json(orient='records'))\n",
    "r = api.post('serventias',serventia_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Competencias"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "competencias = TJData.read_csv(TJData.csv['competencia'], usecols=['COD_COMP', 'DESC_COMP', 'DESC_RES'])\n",
    "competencias = competencias.rename(columns=lambda x: str(x).lower())\n",
    "competencia_json = json.loads(competencias.to_json(orient='records'))\n",
    "r = api.post('competencias',competencia_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipo Personagem"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tp_pers = TJData.read_csv(TJData.csv['tipo_personagem'], usecols=['COD_TIP_PERS', 'DESC_PERS', 'TIPO_PART','COD_TIP_PERS_INV'])\n",
    "tp_pers.COD_TIP_PERS_INV = nan_to_int(tp_pers,'COD_TIP_PERS_INV')\n",
    "tp_pers = tp_pers.rename(columns=lambda x: str(x).lower())\n",
    "tp_pers = tp_pers.sort_values(by='cod_tip_pers', ascending=True)\n",
    "# prepara json para post de tipo personagem\n",
    "tp_pers_post = json.loads(tp_pers.drop(columns=['cod_tip_pers_inv']).to_json(orient='records'))\n",
    "# prepara json para atualizar as referencias circulares\n",
    "tp_pers_patch = json.loads(pd.DataFrame(tp_pers[['cod_tip_pers','cod_tip_pers_inv']]).to_json(orient='records'))\n",
    "r = api.post('tipospersonagem',tp_pers_post)\n",
    "bulk_patch = []\n",
    "for tp in tp_pers_patch:\n",
    "    resp = api.patch('tipospersonagem',tp['cod_tip_pers'],tp)\n",
    "    bulk_patch.append(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assunto"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "assunto = TJData.read_csv(TJData.csv['assunto'], usecols=['COD_ASSUNTO', 'COD_ASSUNTO_PAI', 'DESCR'])\n",
    "assunto = assunto.rename(columns=lambda x: str(x).lower())\n",
    "assunto = assunto.rename(columns={'cod_assunto_pai':'assunto_pai'})\n",
    "assunto = assunto.sort_values(by='cod_assunto', ascending=True)\n",
    "assunto.assunto_pai = nan_to_int(assunto,'assunto_pai')\n",
    "# cria instancias de assuntos\n",
    "assunto_post = assunto.drop(columns=['assunto_pai'])\n",
    "assunto_post = json.loads(assunto_post.to_json(orient='records'))\n",
    "r = api.post('assuntos', assunto_post)\n",
    "# atualiza as referencias circulares\n",
    "assunto_patch = pd.DataFrame(assunto[['cod_assunto','assunto_pai']])\n",
    "assunto_patch = json.loads(assunto_patch.to_json(orient='records'))\n",
    "bulk_patch = []\n",
    "for assunto in assunto_patch:\n",
    "    resp = api.patch('assuntos',assunto['cod_assunto'],assunto)\n",
    "    bulk_patch.append(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classe"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "classe = TJData.read_csv(TJData.csv['classe'], usecols=['COD_CLASSE', 'DESCR', 'COD_CLASSE_PAI','COD_PERS_ATIVO','COD_PERS_PASSIVO'])\n",
    "classe = classe.rename(columns=lambda x : str(x).lower())\n",
    "classe = classe.rename(columns={'cod_classe_pai':'classe_pai'})\n",
    "classe = classe.sort_values(by='cod_classe', ascending=True)\n",
    "classe.classe_pai = nan_to_int(classe,'classe_pai')\n",
    "classe.cod_pers_ativo = nan_to_int(classe,'cod_pers_ativo')\n",
    "classe.cod_pers_passivo = nan_to_int(classe,'cod_pers_passivo')\n",
    "classe_post = json.loads(pd.DataFrame(classe[['cod_classe','descr']]).to_json(orient='records'))\n",
    "classe_patch = json.loads(pd.DataFrame(classe[['cod_classe','classe_pai','cod_pers_ativo','cod_pers_passivo']]).to_json(orient='records'))\n",
    "r = api.post('classes',classe_post)\n",
    "bulk_patch = []\n",
    "for classe in classe_patch:\n",
    "    resp = api.patch('classes',classe['cod_classe'],classe)\n",
    "    bulk_patch.append(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classe assunto"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bulk_post = []\n",
    "for classe_assunto in TJData.read_csv(TJData.csv['classe_assunto_2'], usecols=['COD_CLASSE','COD_ASSUNTO'], chunksize=10000):\n",
    "    classe_assunto = classe_assunto.rename(columns={'COD_CLASSE':'classe','COD_ASSUNTO':'assunto'})\n",
    "    classe_assunto = json.loads(classe_assunto.to_json(orient='records'))\n",
    "    r = api.post('classesassuntos',classe_assunto)\n",
    "    bulk_post.append(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tratamento de Erros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtra erros de criação\n",
    "def errors_on_create_reponses(responses): \n",
    "    return list(filter(lambda x: x.status_code != 201,bulk_post))\n",
    "\n",
    "# converte para json o conteudo retornado da criação em lotes\n",
    "def json_to_list_errors(error_responses):\n",
    "    return list(map(lambda x: json.loads(x.text),error_201))\n",
    "\n",
    "# filtra os registros que não tiveram problema\n",
    "def remove_correct(response_list):\n",
    "    errors = []\n",
    "    for el in response_list:\n",
    "        errors = errors + list(filter(lambda x : x != {}, el))\n",
    "    return errors\n",
    "\n",
    "# gera uma lista a partir da lista de dicionarios do atributo\n",
    "def attribute_errors(errors, attribute):\n",
    "    return list(map(lambda x: x[attribute],errors))\n",
    "\n",
    "# remove duplicados\n",
    "def remove_duplicate(errors):\n",
    "    return set(map(lambda x: x[0],errors))\n",
    "\n",
    "# lista valores de pk que tiveram erro\n",
    "def list_pk_errors(erros):\n",
    "    return list(map(lambda x: re.search(r'(\\d+)',x).group(),errors))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# processa lista de pks de assunto que tiveram erros\n",
    "error_201 = errors_on_create_reponses(bulk_post)\n",
    "error_201 = json_to_list_errors(error_201)\n",
    "errors = remove_correct(error_201)\n",
    "errors = attribute_errors(errors,'assunto')\n",
    "errors = remove_duplicate(errors)\n",
    "errors = list_pk_errors(errors)\n",
    "\n",
    "# obtem a requisão que ocorreu erro\n",
    "response = errors_on_create_reponses(bulk_post)[0]\n",
    "# filtra as linhas  que possuem referencias a pk de assunto inexistentes\n",
    "classe_assunto = pd.DataFrame(json.loads(response.request.body))\n",
    "classe_assunto = df[~df['assunto'].isin(errors)]\n",
    "classe_assunto = json.loads(classe_assunto.to_json(orient='records'))\n",
    "r = api.post('classesassuntos',classe_assunto)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipo movimento"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tp_movimento = TJData.read_csv(TJData.csv['tipo_movimento_2'], usecols=['COD_TIP_MOV', 'COD_TIP_MOV_PAI','DESCR'])\n",
    "tp_movimento = tp_movimento.rename(columns=lambda x : str(x).lower())\n",
    "tp_movimento.cod_tip_mov_pai = nan_to_int(tp_movimento,'cod_tip_mov_pai')\n",
    "tp_movimento = tp_movimento.sort_values(by='cod_tip_mov', ascending=True)\n",
    "tp_movimento_post = json.loads(tp_movimento[['cod_tip_mov','descr']].to_json(orient='records'))\n",
    "tp_movimento_patch = json.loads(tp_movimento[['cod_tip_mov','cod_tip_mov_pai']].to_json(orient='records'))\n",
    "r = api.post('tiposmovimento',tp_movimento_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipo Andamento"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tp_andamento = TJData.read_csv(TJData.csv['tipo_andamento'], usecols=['COD_TIP_AND','DESCR','COD_TIP_MOV'])\n",
    "tp_andamento = tp_andamento.rename(columns=lambda x : str(x).lower())\n",
    "tp_andamento.cod_tip_mov = nan_to_int(tp_andamento,'cod_tip_mov')\n",
    "tp_andamento = json.loads(tp_andamento.to_json(orient='records'))\n",
    "r = api.post('tiposandamento',tp_andamento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urllib3.connectionpool DEBUG    Starting new HTTP connection (1): 0.0.0.0:8000\n",
      "urllib3.connectionpool DEBUG    http://0.0.0.0:8000 \"POST /api/cargos/ HTTP/1.1\" 201 1209\n"
     ]
    }
   ],
   "source": [
    "cargo = TJData.read_csv(TJData.csv['cargo'])\n",
    "cargo = cargo.rename(columns=lambda x: str(x).lower())\n",
    "cargo = json.loads(cargo.to_json(orient='records'))\n",
    "r = api.post('cargos', cargo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urllib3.connectionpool DEBUG    Starting new HTTP connection (1): 0.0.0.0:8000\n",
      "urllib3.connectionpool DEBUG    http://0.0.0.0:8000 \"GET /api/cargos/2 HTTP/1.1\" 301 0\n",
      "urllib3.connectionpool DEBUG    Resetting dropped connection: 0.0.0.0\n",
      "urllib3.connectionpool DEBUG    http://0.0.0.0:8000 \"GET /api/cargos/2/ HTTP/1.1\" 200 45\n",
      "chardet.charsetprober DEBUG    EUC-TW Taiwan prober hit error at byte 38\n",
      "chardet.charsetprober DEBUG    utf-8  confidence = 0.505\n",
      "chardet.charsetprober DEBUG    SHIFT_JIS Japanese confidence = 0.01\n",
      "chardet.charsetprober DEBUG    EUC-JP Japanese confidence = 0.01\n",
      "chardet.charsetprober DEBUG    GB2312 Chinese confidence = 0.01\n",
      "chardet.charsetprober DEBUG    EUC-KR Korean confidence = 0.01\n",
      "chardet.charsetprober DEBUG    CP949 Korean confidence = 0.01\n",
      "chardet.charsetprober DEBUG    Big5 Chinese confidence = 0.01\n",
      "chardet.charsetprober DEBUG    EUC-TW not active\n",
      "chardet.charsetprober DEBUG    windows-1251 Russian confidence = 0.01\n",
      "chardet.charsetprober DEBUG    KOI8-R Russian confidence = 0.01\n",
      "chardet.charsetprober DEBUG    ISO-8859-5 Russian confidence = 0.01\n",
      "chardet.charsetprober DEBUG    MacCyrillic Russian confidence = 0.01\n",
      "chardet.charsetprober DEBUG    IBM866 Russian confidence = 0.01\n",
      "chardet.charsetprober DEBUG    IBM855 Russian confidence = 0.01\n",
      "chardet.charsetprober DEBUG    ISO-8859-7 Greek confidence = 0.01\n",
      "chardet.charsetprober DEBUG    windows-1253 Greek confidence = 0.01\n",
      "chardet.charsetprober DEBUG    ISO-8859-5 Bulgairan confidence = 0.01\n",
      "chardet.charsetprober DEBUG    windows-1251 Bulgarian confidence = 0.01\n",
      "chardet.charsetprober DEBUG    TIS-620 Thai confidence = 0.19626611565608917\n",
      "chardet.charsetprober DEBUG    ISO-8859-9 Turkish confidence = 0.4611047517683042\n",
      "chardet.charsetprober DEBUG    windows-1255 Hebrew confidence = 0.0\n",
      "chardet.charsetprober DEBUG    windows-1255 Hebrew confidence = 0.0\n",
      "chardet.charsetprober DEBUG    windows-1255 Hebrew confidence = 0.0\n",
      "chardet.charsetprober DEBUG    utf-8  confidence = 0.505\n",
      "chardet.charsetprober DEBUG    SHIFT_JIS Japanese confidence = 0.01\n",
      "chardet.charsetprober DEBUG    EUC-JP Japanese confidence = 0.01\n",
      "chardet.charsetprober DEBUG    GB2312 Chinese confidence = 0.01\n",
      "chardet.charsetprober DEBUG    EUC-KR Korean confidence = 0.01\n",
      "chardet.charsetprober DEBUG    CP949 Korean confidence = 0.01\n",
      "chardet.charsetprober DEBUG    Big5 Chinese confidence = 0.01\n",
      "chardet.charsetprober DEBUG    EUC-TW not active\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'cod_carg': 2, 'descr': 'Analista Judiciário'}, <Response [200]>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.get('cargos',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<QuerySet [<Cargo: Cargo object (2)>, <Cargo: Cargo object (15)>, <Cargo: Cargo object (4)>, <Cargo: Cargo object (14)>, <Cargo: Cargo object (9)>, <Cargo: Cargo object (17)>, <Cargo: Cargo object (12)>, <Cargo: Cargo object (7)>, <Cargo: Cargo object (20)>, <Cargo: Cargo object (26)>, <Cargo: Cargo object (10)>, <Cargo: Cargo object (8)>, <Cargo: Cargo object (23)>, <Cargo: Cargo object (3)>, <Cargo: Cargo object (19)>, <Cargo: Cargo object (98)>, <Cargo: Cargo object (24)>, <Cargo: Cargo object (21)>, <Cargo: Cargo object (22)>, <Cargo: Cargo object (25)>, '...(remaining elements truncated)...']>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tj.Cargo.objects.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
